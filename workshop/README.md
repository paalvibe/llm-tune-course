# Topptur: LLM tuning hands-on training

## Preparation

1. Each student should checkout this repo in databricks, under their personal repos folder.
2. The teacher should prepare enough `g5.xlarge` GPU-clusters for all students.
3. The teacher can also prepare the extra tasks under 02-extra-tasks.

## Instructions

1. The teacher should run through the `00-TEACHER-prep` folder once.
2. The students should read through the `00-TEACHER-prep` folder without running it.
3. The teacher should run through the 01-small-tuning notebooks and tune the model once (it takes one hour on a an g5.xlarge aws GPU instance). To tune it, comment in the run_summarization bash cell.
4. The students should run through the 01-small-tuning notebooks, without running the run_summarization bash cell.
5. The students should run through the 02-large-tuning notebooks, without running the run_summarization bash cell.
6. The students do the tasks in the 03-lang-chain folder.